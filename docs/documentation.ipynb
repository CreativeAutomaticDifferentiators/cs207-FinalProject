{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OttoDiff Documenation: Group 6\n",
    "\n",
    "**Team Members:**  \n",
    "Kyra Ballard - *S.M. Applied Computation*  \n",
    "Paul-Emile Landrin - *S.M. Applied Computation*  \n",
    "Yaoyang Lin - *S.M. Applied Computation*  \n",
    "Dan Park - *MS/MBA*  \n",
    "\n",
    "CS 207 - Systems Development for Computational Science (Fall 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project aims to provide a working library for automatic differentiation. Automatic differentiation is a method for numerically calculating the derivative of a function. Derivative calculations are essential in many modern numerical algorithms, and automatic differentiation is one method that can quickly compute derivatives by executing elementary functions via the chain rule. \n",
    "\n",
    "Compared to other differentiation methods (such as symbolic/analytic differentiation or numerical differentiation), automatic differentiation boasts more efficient calculations and high accuracy to working precision. Symbolic differentiation requires mathematical expressions of derivatives to be kept throughout the process, which can lead to inefficient code. Numerical differentation is heavily reliant on an optimal value of `h`--too small or big values of `h` can lead to large floating point errors or inaccuracy, respectively. Therefore, automatic differentic differentiation provides efficiency and accuracy advantages (in most cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "As mentioned above, automatic differentiation (AD) revolves around the fact that any complex function can be decomposed into elementary functions. As such, derivatives for a complex function can be computed from the derivatives of each elementary function, via the chain rule:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{df_1}{dx} = \\frac{df_1}{df_2} \\frac{df_2}{df_3} ... \\frac{df_{n}}{dx}\n",
    "\\end{equation}\n",
    "\n",
    "There are two main methods for automatic differentiation: 1) forward-mode and 2) reverse-mode. In forward-mode, the derivative is calculated \"inside-out\" (from the independent variable to the outermost function). In reverse-mode, the derivative is calculated \"outside-in\". We implement both forward-mode and reverse-mode in our project.\n",
    "\n",
    "AD keeps track of the elementary functions, their derivatives, and the value of the derivative at the given x-value. These functions and values can be summarized in a table called the evaluation trace. By evaluating the derivative at each step, we can compute the value of the derivative at the given x-value. The composition of elementary functions can also be visualized in an evaluation graph.\n",
    "\n",
    "To illustrate the above, we present an example: \n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = 2x^2 + \\sin(y)\n",
    "\\end{equation}\n",
    "\n",
    "at $(x,y) = (4, \\frac{\\pi}{2})$\n",
    "\n",
    "This equation has the following evaluation trace:\n",
    "\n",
    "| Trace   | Elementary Function      | Current Value           | Elementary Function Derivative       | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  |\n",
    "| :---: | :-----------------: | :-----------: | :----------------------------: | :-----------------:  | :-----------------: |\n",
    "| $x_{1}$ | $x_{1}$                  | $4$                     | $\\dot{x}_{1}$                        | $1$ | $0$ |\n",
    "| $x_{2}$ | $x_{2}$                  | $\\dfrac{\\pi}{2}$        | $\\dot{x}_{2}$                        | $0$ | $1$ |\n",
    "| $x_{3}$ | $x_{1}^2$                | $16$                    | $2x_1\\dot{x}_{1}$                    | $8$ | $0$ |\n",
    "| $x_{4}$ | $2x_{3}$                 | $32$                    | $2\\dot{x}_{3}$                       | $16$| $0$ |\n",
    "| $x_{5}$ | $\\sin(x_{2})$            | $1$                     | $\\cos(x_2)\\dot{x}_{2}$               | $0$ | $1$ |\n",
    "| $x_{6}$ | $x_{4} + x_{5}$          | $33$                    | $\\dot{x}_{4}+\\dot{x}_{5}$            | $16$| $1$ |\n",
    "\n",
    "The function can be visualized with the following evaluation graph:\n",
    "![evaluation_graph.png](./image_m2/evaluation_graph.png)\n",
    "\n",
    "In execution, forward-mode AD actually computes $\\nabla f \\cdot p$. However, this needs to be calculated for each seed vector (there is one seed vector for each input variable). Therefore, the computational complexity of forward-mode AD is proportional to the number of input variables. Forward-mode automatic differentiation is more efficient for functions where $ m >> n$ for $f:\\mathbb{R}^{n} \\mapsto \\mathbb{R}^{m} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background for Extension Feature: Reverse Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our advanced/extension feature, we implemented the reverse mode. We believe that this is a good complement to the forward mode so that we can compare the efficiency in computing them, under different conditions. Theretically, the two modes should yield the same answer (the true derivative value at a given point) to machine precision.\n",
    "\n",
    "As mentioned above, forward-mode calculates the derivative \"inside-out\" (from the independent variable to the outermost function) using the chain rule, and the reverse-mode calculates the derivative \"outside-in\". For example, consider the following function:\n",
    "\n",
    "\\begin{equation}\n",
    "y = f(g(h(x)))\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the function above would be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dw}\\frac{dw}{dx}\n",
    "\\end{equation}\n",
    "\n",
    "where $w = h(x)$ and $u = g(h(x))$.\n",
    "\n",
    "In forward mode, the the sweep starts from calculating $\\frac{dw}{dx}$, then $\\frac{du}{dw}$, then $\\frac{dy}{du}$ (inside-out). Forward-mode is advantageous in cases where $ m >> n$ for $f:\\mathbb{R}^{n} \\mapsto \\mathbb{R}^{m} $ for reasons described before. \n",
    "\n",
    "In reverse-mode, there are two sweeps. The first sweep is a forward sweep that calculates the function values and partial derivatives at each intermediate step. The second sweep is a reverse sweep that starts from $\\frac{dy}{du}$, then $\\frac{du}{dw}$, then $\\frac{dw}{dx}$ (outside-in) to compute the numerical values of the partial derivatives. We start with:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x}_{N} = \\frac{\\partial f}{\\partial x_N} = 1\n",
    "\\end{equation}\n",
    "\n",
    "Then we start sweeping inwards:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x}_{N-1} = \\frac{\\partial f}{\\partial x_N} \\frac{\\partial x_N}{\\partial x_{N-1}}\n",
    "\\end{equation}\n",
    "\n",
    "Reverse mode, different from the forward mode, does not compute derivatives simultaneously. Instead, it requires the storage of intermediate variables $(u, w,$ etc.$)$ and backpropagate the gradient from the opposite version of the operation flow. Therefore, the computational complexity is proportional to the number of functions (i.e. for cases in which $ m << n$ for $f:\\mathbb{R}^{n} \\mapsto \\mathbb{R}^{m} $)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use *OttoDiff*\n",
    "\n",
    "### How to install\n",
    "\n",
    "#### Use pip to install\n",
    "\n",
    "Our package has been posted to **PyPI** as `OttoDiff` to make installation easy for users. Users are allowed to install the package by typing the following command.\n",
    "\n",
    "`pip install OttoDiff`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `requirements.txt` contains dependent packages that the user needs to be able to run OttoDiff (such as numpy>=1.14.3).\n",
    "\n",
    "#### Install manually in Virtual Environment  (CONDA)\n",
    "\n",
    "User are also allowed to install the package following these steps on the command line:\n",
    "0. Download the yaml file `ottodiff_env.yaml`, found in the root directory of the project repository.\n",
    "1. Set up a virtual environment: `conda env create --file=ottodiff_env.yaml`\n",
    "2. Activate the virtual environment: `conda activate ottodiff_env`\n",
    "3. Clone the repo by `git clone https://github.com/CreativeAutomaticDifferentiators/cs207-FinalProject.git`\n",
    "4. `cd cs207-FinalProject`\n",
    "5. Install all of the dependencies packages required for this project: `pip install -r requirements.txt`\n",
    "6. install OttoDiff by : `pip install OttoDiff`\n",
    "7. Quit by : `deactivate`\n",
    "\n",
    "### Demo: How to use OttoDiff to calculate derivatives\n",
    "\n",
    "The user should first import the package to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from OttoDiff.forward import *\n",
    "from OttoDiff.reverse import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a basic demo about how to use the Package for three different cases:\n",
    "### Univariate derivative calculation\n",
    "\n",
    "The core class of the forward mode is `Variable`, which stores both the value and the partial derivatives of a variable (expression). User can define any functions f(x) which made by any operations of x. And the derivative and value of f(x) would be automaticly calculated and been stored in the `val` and `der` attribute of the `Variable` during the forward process. Because the operator is overloaded and a new return \n",
    "\n",
    "For example, user create an function $f(x) = \\ln(x) + 3x + 1$, we can manually calculate its derivative $f'(x) = \\frac{1}{x} + 3$. When $x = 2$, we know that $f(x) = \\ln(2) + 7 = 7.6931$ and $f'(x) = 3.5$.\n",
    "\n",
    "The calculation would be much easier if our package is used. The user just need to create the `Variable` x first by `x = Variable(x_val)`, use the operations of `Variable` x to create a function f(x). Then the value and derivative would be automatically stored in the attributes of f(x) and user can easily check them by print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 7.693147180559945 der: 3.5\n"
     ]
    }
   ],
   "source": [
    "x = Variable(2)\n",
    "f_x = np.log(x) + 3*x + 1\n",
    "print(f_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutivariate derivative calculation\n",
    "\n",
    "The current implementation also support mutivariate derivative calculation. Most part usage is similar but the user has to initialize the partial derivatives for this usage. \n",
    "\n",
    "For example, user create an function $f(x, y) = \\ln(x) + 3y + 1$, we can manually calculate its partial derivative $\\frac{\\partial f}{\\partial x} = \\frac{1}{x}$ and $\\frac{\\partial f}{\\partial y} = 3$. When $x = 2, y = 1$, we know that $f(x) = \\ln(2) + 4 = 4.6931$ and $\\frac{\\partial f}{\\partial x} = 0.5$ and $\\frac{\\partial f}{\\partial y} = 3$.\n",
    "\n",
    "With the help of createVariables of AutoDiffFwd() class, user are allowed to create all the the variables they want by this function. The user does not need to specify the der of Variable like der = [1,0], the AutoDiffFwd would automatically figure it out and assign an der to each Variable. In the AutoDiffFwd class, der of each Variable is a numpy array (length = number of variables in the equations). Each element of the array represents a unique variable. And finally the package will also return the partial derivative as the same order the class defined, in this case $[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 4.693147180559945 der: [0.5 3. ]\n"
     ]
    }
   ],
   "source": [
    "autodiff = AutoDiffFwd()\n",
    "x, y = autodiff.createVariables(['x', 'y'], [2, 1])\n",
    "f = np.log(x) + 3*y + 1\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutivariate Vector Functions\n",
    "\n",
    "The forward mode of the package also support the the vector function derivative calculation. In order to use that functionality, the user should first create several non-vector functions following the guidence above and use `autodiff.createVectorFunction` to combine them into a vector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vals: [2 3] jacobian: [[2. 1.]\n",
      " [1. 1.]]\n",
      "[[2. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "autodiff.reset()\n",
    "x, y = autodiff.createVariables(['x', 'y'], [1, 2])\n",
    "f1 = x * y\n",
    "f2 = x + y\n",
    "vec_f = autodiff.createVectorFunction([f1, f2])\n",
    "print(vec_f)\n",
    "print(vec_f.get_jacobian())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Demo]: How to use OttoDiff for root-finding algorithm (e.g. Newton's method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEaCAYAAAASSuyNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvlJK+Qgt+Eop5SO04CullI/4f+gLAqRYivBbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def newtons_method(f, x, tolerance=1e-6):\n",
    "    '''f is the function f(x)'''\n",
    "    while True:\n",
    "        df = f(Variable(x))\n",
    "        x1 = x - (f(x) / df.der)\n",
    "        t = abs(x1 - x)\n",
    "        if t < tolerance:\n",
    "            break\n",
    "        x = x1\n",
    "    return x\n",
    "\n",
    "f = lambda x: x**2-2*x+1\n",
    "sol = newtons_method(f, -1)\n",
    "\n",
    "xs = np.linspace(-2,4,100)\n",
    "plt.plot(xs, f(xs), label=\"f\")\n",
    "plt.scatter(sol, f(sol), label=\"Root\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Use Newton's Method to find root for $x^2 - 2x + 1$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Extension]: Reverse Mode\n",
    "\n",
    "Similar to the forward mode, the reverse mode allow us to calculate the derivative easily. User is allowed to create a node Variable with the value they want at first, For example, they can create variable x with value 3. Then they can just use the function in the package `find_df_dx` to find the the patial derivative of f with respect to x. The reverse mode supports all the operations in the forward mode and also mulltivariate cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "x = VariableNode(2)\n",
    "f = np.log(x) + 3 * x + 1\n",
    "\n",
    "print(find_df_dx(f, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5403023058681398\n"
     ]
    }
   ],
   "source": [
    "a = VariableNode(1)\n",
    "b = VariableNode(2)\n",
    "\n",
    "f2 = np.sin(a) + np.sqrt(b)\n",
    "print(find_df_dx(f2, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Extension]: Visualization of the Computaional Graph\n",
    "\n",
    "Another extension we implemented is the visualization of the reverse mode Computaional Graph. As the function get complicated, it gets much harder to confirm if the computaional architectures are constructed properly. Hence, visualization of the computational graph could be very helpful. Below are the code to generate the computaional graph visualization. It is pretty straightforward to use by calling `visualize(function_node)`, a new computational graph would been stored in the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from OttoDiff.visualization import *\n",
    "\n",
    "x = VariableNode(2)\n",
    "f = np.log(x) + 3 * x + 1\n",
    "visualize(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Computaional Graph generated from the above code, as we can see, the user can easily check about the value, operation, and the relationship between the node.\n",
    "![comp_graph.png](./image_m2/comp_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "```\n",
    "cs207-FinalProject/\n",
    "\tOttoDiff/\n",
    "        forward.py\n",
    "        reverse.py\n",
    "        visualization.py\n",
    "        tests/\n",
    "            test_forward.py\n",
    "            test_reverse.py\n",
    "\tdocs/\n",
    "        documentation.ipynb\n",
    "        milestone1.md\n",
    "        milestone2.ipynb\n",
    "        image_m2/\n",
    "            evaluation_graph.png\n",
    "\t.travis.yml\n",
    "    ottodiff_env.yaml\n",
    "\tREADME.md\n",
    "\trequirements.txt\n",
    "\tsetup.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory structure\n",
    "\n",
    "1 OttoDiff/\n",
    "\n",
    "The `OttoDiff` directory contains the different code modules. It contains the forward-mode implementation, the reverse-mode implementation, the code for visualization, and the corresponding tests in the `tests` folder. \n",
    "\n",
    "2 docs/\n",
    "\n",
    "The `docs` directory contains the documentations correpond to each milestone. The final documentation (this document) and Milestone 2 documentation were submitted as a python notebook, to demonstrate in-line demos for how to use our package. There is also a `evaluation_graph.png` image that is referenced in this documentation.\n",
    "\n",
    "3 ./\n",
    "\n",
    "The root directory of `cs207-FinalProject` contains the following additional files for installation and configuration:\n",
    "* `.travis.yml` - contains our configuation for TravisCI and CodeCov\n",
    "* `ottodiff_env.yaml` - contains the yaml configuration file to setup a virtual environment.\n",
    "* `README.md` - contains basic information about our project (members, group number, etc.). Also has badges to help us easily understand code coverage and build pass/fail info.\n",
    "* `requirements.txt` - outlines dependencies on other packages for our project. Mainly `python`, `numpy`, and `math`.\n",
    "* `setup.py` - a python file that configures installation with `pip`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic modules and functionality\n",
    "\n",
    "There are three implemented modules: `forward`, `reverse`, and `visualization`.\n",
    "\n",
    "The `forward` module contains the implementaion of class `Variable`, which is the core class for calculating the value and derivative of any variable (expression) by overloading the basic operators. A second class `VectorVariable` is an extension of the `Variable` class that allows for vector functions. We also included one extra class in this module: `AutoDiffFwd`, which serves as a management class to handle the case of mutiple variables and vector functions.\n",
    "  \n",
    "The `reverse` module includes classes used for reverse mode. The class proposed to include in this module is `VariableNode`. It would be used to build the computational graph required by reverse mode.\n",
    "\n",
    "The `visualization` module uses the the `graphviz.Digraph` package to generate computional graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Our tests are contained in test directory in modules.\n",
    "\n",
    "test_forward.py: It includes tests for scalar and vector functions of Variable such as (+, -, *, /) is successfully overloaded by own implementation. It also make sure that the exponential and trig functions of numpy is also overloaded by our implementation. It checks that the calculation of expression of variable and derivative is correct by manual calculation. It also ensure that an useful error message is raised when user type in invalid input.\n",
    "\n",
    "test_reverse.py: This is a similar test file for the reverse-mode. The instantiation of the VariableNode class is different due to the nature of reverse-mode.\n",
    "\n",
    "We used Travis CI and CodeCov to keep track of the testing progress. We have already set up these integrations, which can be visualized by the badges in the README.md. It shows that our tests covers 97% of the code now.\n",
    "\n",
    "If user are intrested in adding more tests and run their self-defined test, they can add them in the file test_forward.py and run the command: `python -m pytest ./forward/tests/test_forward.py`. This can be done for the reverse-mode as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging\n",
    "\n",
    "We follow instructions on https://packaging.python.org/tutorials/packaging-projects/ to package our code and distribute it on PyPI. We use `twine upload dist/*` to upload our package. Details about how to install the package are discussed in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the forward mode, we implemented Variable class, VectorVariable class, AutoDiffFwd class. \n",
    "\n",
    "`Variable` : An object of this class as two attributes val and der which stands for the value and the derivatives. We can then do some operations on this variable like addition, multiplication, or composition. This will generate another variable with updated values and derivatives.\n",
    "\n",
    "`VectorVariable` : An object of this class is a special case of the Variable. For VectorVariable, the attribute val is an array instead of number, the attribute der is a matrix instead of a list. der in this class is acturally Jacobian matrix, which is the matrix of partial derivatives of variables for differentiation. Allowing val to be array can also do the same things but we decide to define another class in case there are some new features need to been developed for the vector in the future.\n",
    "\n",
    "`AutoDiffFwd` : Previously, if we want to calculate the gradients of an expression with several variables (function with several inputs), we need to specify the derivative as an array whose only coefficient that is non-zero is the coefficient of the index of that variable. For example, if you want to use two variables you need to declare the variables as follow, the two derivatives that you would put in the Variable will be the arrays [1,0] and [0,1]. This class is trying to avoid this problem due to the encapsulation reason. The user can now work with multiple variables without worrying about the initialization of derivatives.\n",
    "\n",
    "In the reverse mode, which is part of our extension, the core classes are VariableNode class and Constant class.\n",
    "\n",
    "`VariableNode` : The object can been viewed as as an node in the computational graph. During the computational process, any intermediate result would be stored as a VariableNode and been added into the children list of the parent nodes. For example, z = x + y, a new VariableNode z is created and is the child of x and y.\n",
    "\n",
    "`Constant` : The object is a special case of the VariableNode. A Constant VariableNode would always have 1 as gradient.\n",
    "\n",
    "## Core data structures\n",
    "\n",
    "List and Numpy array: Array is the data structure we heavily used. Use a numpy array which length equal to the number of variables to store partial derivatives. We also use a 2D numpy array which shape is (vector length, variable number).\n",
    "\n",
    "Dictionary and hashmap: We use the data structure in our implementaion for manangement of mutiple Varibles and vector functions. It is used to record the mapping relationship between the index of variable in the list and its name. For example, for dict var2index = {x:0, y:1, z:2} and partial derivatives [1, 2, 3]. We know that $\\frac{\\partial f}{\\partial x} = 1$, $\\frac{\\partial f}{\\partial y} = 2$, $\\frac{\\partial f}{\\partial z} = 3$\n",
    "\n",
    "Graph/Tree: We inplicit use that data structure in the reverse mode. The key idea of the reverse mode is to build a computational graph during the intermediate computaion. And the the gradient of each node can be found recusively due to the graph/tree property.\n",
    "\n",
    "\n",
    "## Core classes\n",
    "\n",
    "There are 5 classes in our implementation. Variable, VectorVariable, AutoDiffFwd are our core classes in the forward mode. VariableNode and Constant are our core classes in the reverse mode.\n",
    "### Variable\n",
    "\n",
    "#### Important attributes\n",
    "\n",
    "The core class `Variable` has two important attributes:\n",
    "\n",
    "val: int/float, value of variables for differentiation\n",
    "     \n",
    "der: int/float or np.array if mutiple variable, derivative (partial derivatives) of variables for differentiation. If it is an array, each entry represents an partial derivatives [$\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial y}$, $\\frac{\\partial f}{\\partial z}$, ...]\n",
    "\n",
    "#### Methods\n",
    "\n",
    "The following methods is implemented for the Variable class to overload the original operators:\n",
    "\n",
    "`__init__`: initialize a Variable class object, user inputed value is required, which stored as float/int for current implementation (would extend to array in the future). Derivatives initialization is optional, It would be setted to 1 if the user do not specify about it.\n",
    "\n",
    "`__eq__`: overload the comparator to make sure two variables are equal if and only if their val and der are equal.\n",
    "\n",
    "`__ne__`: overload the comparator to compare two variables.\n",
    "\n",
    "`__neg__`: overload negitive function\n",
    "\n",
    "`__str__`: overload the defult str method, by calling `print(self)`, an help message `val: [value] der: [Derivatives]` would be printed.\n",
    "\n",
    "`__add__`: overload add function to handle addition of two Variable class objects or addition of Variable and a number.\n",
    "\n",
    "`__radd__`: right addition similar to `__add__`.\n",
    "\n",
    "`__sub__`: overload minus function to handle subtraction of two Variable class objects or subtraction of Variable and a number.\n",
    "\n",
    "`__rsub__`: right subtraction similar to `__sub__`.\n",
    "\n",
    "`__mul__`: overload multiplication function to handle multiplication of two Variable class objects or multiplication of Variable and a number.\n",
    "\n",
    "`__rmul__`: right multiplication similar to `__mul__`.\n",
    "\n",
    "`__truediv__`: overload division function to handle division of two Variable class objects or division of Variable and a number.\n",
    "\n",
    "`__rtruediv__`: right division similar to `__truediv__`.\n",
    "\n",
    "`logistic`: caculate the direvative of Logistic function.\n",
    "\n",
    "\n",
    "We also overloaded several function of the numpy library, these functions can be called by `np.sin(Variable)`\n",
    "\n",
    "`sin()`\n",
    "\n",
    "`cos()`\n",
    "\n",
    "`tan()`\n",
    "\n",
    "`sinh()`\n",
    "\n",
    "`cosh()`\n",
    "\n",
    "`tanh()`\n",
    "\n",
    "`arcsin()`\n",
    "\n",
    "`arccos()`\n",
    "\n",
    "`arctan()`\n",
    "\n",
    "`sqrt()`\n",
    "\n",
    "`exp()`\n",
    "\n",
    "`log()`\n",
    "\n",
    "\n",
    "\n",
    "### VectorVariable\n",
    "\n",
    "#### Important attributes\n",
    "The class `VectorVariable` is the Child class of `Variable` which has two similar important attributes:\n",
    "\n",
    "val: np.array, values of variables for differentiation\n",
    "     \n",
    "der: 2D np.array, der here stands for Jacobian matrix, which is a matrix of partial derivatives of variables for differentiation. Each row of the matrix represents an partial derivatives of an function (∂f1/∂x, ∂f1/∂y, ∂f1/∂z, ...). Each column represents an partial derivatives with respect of one variable (∂f1/∂x, ∂f2/∂x, ∂f3/∂x, ...)\n",
    "\n",
    "#### Methods\n",
    "\n",
    "Most of overloading operators are inherited from the parent class `Variable` to make the vector function still support basic operations such as adding an constant. However, the Vector Variable has slight difference so that some method need to be overloaded to match the property of Vector.\n",
    "\n",
    "`__str__`: Since val is an array and der is jacobian in in VectorVariable, the str method is slightly changed.\n",
    "\n",
    "`get_jacobian`: jacobian is exactly the same as der attribure in VectorVariable.\n",
    "\n",
    "`__eq__`: Since the val is changed from int/float to array, comparator should be changed to support array comparation. Again two VectorVariables are equal if and only if their value array and jacobian matrix are exactly the same.\n",
    "\n",
    "`__ne__`: Overload the not equal comparator.\n",
    "\n",
    "### AutoDiffFwd\n",
    "\n",
    "#### Important attributes\n",
    "\n",
    "The class `AutoDiffFwd` only has one important attribute:\n",
    "var2idx: dictionary, is used to track the variable information about the computation. For example, we can know how many variables are been created in this computation by the length of var2idx. We can know all names of the variables and the index for each variable in the derivatives. For example, given an partial derivatives [1, 2, 3] and var2idx = {'x':0, 'y':1, 'z', 2}, we know the first entry represents the partial derivative with respect to x.\n",
    "\n",
    "#### Methods\n",
    "`createVariables`: Used for creating all variables needed in the following computation so that the user do not need to specify the derivatives in the multivariate computation.\n",
    "\n",
    "`reset`: Clear the variables recorded in order to be reused in new computation in order to reuse the class in the future computation.\n",
    "\n",
    "`createVectorFunction`: Used for creating a vector function from a list of non-vector functions.\n",
    "\n",
    "`getVerboseInformation`: Print out some helpful information about the given variable. For example, whether the variable is vector function or mutivariable function.\n",
    "\n",
    "\n",
    "### VariableNode\n",
    "\n",
    "#### Important attributes\n",
    "\n",
    "val: stores the value of the node in the graph\n",
    "\n",
    "_grad : stores the gradient of the final output node f with respect to current node (∂f/∂(x))\n",
    "\n",
    "parents : stores the parents of the current node in the computaional graph, used for the visualization part.\n",
    "\n",
    "operation: stores the operation computed in the current node.\n",
    "\n",
    "children: stores the children of the current node in the computaional graph, used for compute the grad.\n",
    "\n",
    "idCounter: use to identify each node in the graph, for the visualization purpose\n",
    "\n",
    "name: name of the node is composed of node id, operation, value, used for the visualization part.\n",
    "        \n",
    "\n",
    "#### Methods\n",
    "The key method of find the derivative in the VariableNode is `compute_grad`.\n",
    "\n",
    "`compute_grad` : The gradient of the current node is been computed from the gradient of the child node by chain rule. Whenever the node gradient value is calculated, it is cached in _grad attribute.\n",
    "\n",
    "VariableNode also support exactly the same number of operations as Forward mode, including basic operations, Trig functions, exponentials and so on. By operator overloading, when the user types the fomula, an computaional graph is been implicitly created and user can visualize the graph by the visualization module of the package.\n",
    "\n",
    "`__neg__`\n",
    "\n",
    "`__str__`\n",
    "\n",
    "`__add__`\n",
    "\n",
    "`__radd__`\n",
    "\n",
    "`__sub__`\n",
    "\n",
    "`__rsub__`\n",
    "\n",
    "`__mul__`\n",
    "\n",
    "`__rmul__`\n",
    "\n",
    "`__truediv__`\n",
    "\n",
    "`__rtruediv__`\n",
    "\n",
    "`logistic`\n",
    "\n",
    "`sin()`\n",
    "\n",
    "`cos()`\n",
    "\n",
    "`tan()`\n",
    "\n",
    "`sinh()`\n",
    "\n",
    "`cosh()`\n",
    "\n",
    "`tanh()`\n",
    "\n",
    "`arcsin()`\n",
    "\n",
    "`arccos()`\n",
    "\n",
    "`arctan()`\n",
    "\n",
    "`sqrt()`\n",
    "\n",
    "`exp()`\n",
    "\n",
    "`log()`\n",
    "\n",
    "### Constant\n",
    "        \n",
    "Constant is just an specific type of VariableNode, it inherit all the attributes and Methods from the parent class. The only difference is that it has a different way to compute gradient.\n",
    "\n",
    "#### Methods\n",
    "`compute_grad` : the gradient of the Constant node is always 1.\n",
    "\n",
    "\n",
    "### Other Methods\n",
    "`visualize`: It is been used to visulize the computational graph we created in the reverse mode computation.\n",
    "\n",
    "`find_df_dx`: In the reverse mode, used to find the partial derivative of f with respect to given x.\n",
    "\n",
    "## External dependencies\n",
    "\n",
    "Numpy: used for mathematical calculations. We rely on the numpy library for implemented the following elementary functions: power, exponential, logarithm, sinus, cosinus, tangente, arcsinus, arccosinus and arctangente. \n",
    "\n",
    "pytest: used to create simple as well as complex functional test cases.\n",
    "\n",
    "TravisCI and Codecov: used for test integration and coverage check.\n",
    "\n",
    "graphviz: used for visulizing the computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we discuss future work that extends beyond the timeframe of this class and applications for how OttoDiff can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps for OttoDiff\n",
    "There were many additional features that we considered implementing if we had additional time for this project. \n",
    "\n",
    "**Gradient Descent to Find Local Maxima/Minima**  \n",
    "\n",
    "One useful tool in mathematics is the ability to iteratively find the local maximum or the minimum of the function. In gradient descent, one iterates through the function by traveling a distance proportional to the derivative, in the negative direction. \n",
    "\n",
    "Gradient descent can be described using the following equation:\n",
    "\\begin{equation}\n",
    "\\mathbf{a}_{n+1} = \\mathbf{a}_n - \\gamma \\nabla F(\\mathbf{a}_n)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\gamma$ is the learning rate (dictates how much to travel in the opposite direction). A next step for OttoDiff would be to create a method for gradient descent that accepts a function, starting point, and the learning rate as inputs, then uses OttoDiff to generate the gradient for the function at the starting point, and iterates until a local minimum was found. We can also find the local maximum of the function by utilizing gradient ascent:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{a}_{n+1} = \\mathbf{a}_n + \\gamma \\nabla F(\\mathbf{a}_n)\n",
    "\\end{equation}\n",
    "\n",
    "**User Interface to Simply the Usability of OttoDiff**\n",
    "\n",
    "Another potential feature we could have implemented was to package the code in a runtime executable so that users can use AutoDiff without running code. Doing so would have simplified the setup and instantiating variables. Additionally, it would have allowed users without Python or computer science background to be able to easily find derivative values for scalar and vector functions of single or many variables. Our visualization feature for evaluation graphs would have integrated well into this executable, as a way to help users visualize what is happening under the UI, on a mathematical level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications for Automatic Differentiation\n",
    "There are many applications of derivatives that would be enabled by the computational efficiency of Automatic Differentiation. The following is one example:\n",
    "\n",
    "**Edge Detection in Image and Video Processing**\n",
    "\n",
    "In the past few years, the number of machine vision applications and the need for real-time image/video processing have increased dramatically. For many applications such as video processing for autonomous vehicles, the computation times must be extremely low in order to prevent accidents. Additionally, with advancing image capture technologies leading to higher resolution pictures and new formats such as 3D video that require have data files, there is a need for highly efficient methods.\n",
    "\n",
    "Take edge detection for example. Edge detection is a fundamental tool in image/video processing that enables object detection and image classification. As a simplified explanation, edge detection works by finding sharp increases or decreases in pixel values, which can indicate the presence of edges. Notice that this is an application of finding points of high derivative values via differentiation! Finding edges significantly simplifies object classification, which then leads to so many applications.\n",
    "\n",
    "Automatic differentiation is a great tool for this application due to its computational efficiency. In autonomous vehicles, the vehicles capture up to 120 images per second, and the reaction times to recognize objects such as other cars or humans need to be as low as possible. Leveraging automatic differentation would be a great choice for image processing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
